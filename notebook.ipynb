{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Ablation Notebook\n",
    "## Investigate Minimal Causal Sets in Transformers\n",
    "This notebook explores how to identify a minimal set of attention heads needed\n",
    "for a small transformer model to reproduce a known induction pattern (IOI). We do this via\n",
    "Monte Carlo sampling by randomly ablating heads and tracking which heads are consistently\n",
    "required for success (important heads), and which heads consistently lead to failure when ablated (exclusion heads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Run this notebook on Google Colab](https://colab.research.google.com/github/thartlep/stochastic-ablation/blob/main/notebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbElDcG1IotL"
   },
   "source": [
    "### Some settings to adjust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to install needed depenencies using pip\n",
    "INSTALL_DEPENDENCIES = False\n",
    "\n",
    "# Dump data for later use (e.g., follow-up notebooks)\n",
    "DUMP_MONTE_CARLO_RESULTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQC_P1SuoILO"
   },
   "outputs": [],
   "source": [
    "if INSTALL_DEPENDENCIES:\n",
    "    # Your code here# Download the requirements.txt file from GitHub\n",
    "    !wget https://raw.githubusercontent.com/thartlep/stochastic-ablation/main/requirements.txt\n",
    "\n",
    "    # Install the packages from the requirements.txt file\n",
    "    %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python modules and load small transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore mostly harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import needed modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "import pickle\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load small model\n",
    "model_name = \"pythia-160m\"\n",
    "model = HookedTransformer.from_pretrained(model_name)\n",
    "n_layers = model.cfg.n_layers\n",
    "n_heads = model.cfg.n_heads\n",
    "head_indices = [(l,h) for l in range(n_layers) for h in range(n_heads)]\n",
    "n_heads_total = len(head_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And define a helper function to run the model with an ablation mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_ablation(sample_mask, head_indices, tokens):\n",
    "    \"\"\"\n",
    "    Given a binary mask and list of head indices, run the model with\n",
    "    the masked heads zeroed out.\n",
    "    \"\"\"\n",
    "    heads_to_ablate = [head_indices[i] for i, keep in enumerate(sample_mask) if not keep]\n",
    "\n",
    "    def hook_ablate(value, hook):\n",
    "        # value shape: (zbatch, seq, n_heads, d_head)\n",
    "        for i, (layer_idx, head_idx) in enumerate(head_indices):\n",
    "            if (layer_idx, head_idx) in heads_to_ablate and f\"blocks.{layer_idx}.attn.hook_z\" == hook.name:\n",
    "                value[:,:,head_idx,:] = 0.0\n",
    "        return value\n",
    "\n",
    "    hooks = [(f\"blocks.{layer}.attn.hook_z\", hook_ablate) for layer, head in heads_to_ablate]\n",
    "    logits = model.run_with_hooks(tokens, return_type=\"logits\", fwd_hooks=hooks)\n",
    "\n",
    "    seq_lens = (tokens != model.tokenizer.pad_token_id).sum(dim=1)\n",
    "\n",
    "    next_logits = logits[torch.arange(len(prompts)), seq_lens, :]\n",
    "    pred_ids = next_logits.argmax(dim=-1)\n",
    "    predicted_answers = [model.to_string(pid.unsqueeze(0)) for pid in pred_ids]\n",
    "\n",
    "    return predicted_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now setup some test prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_data = [\n",
    "    (\"Tom eats apples. Tom eats\", \" apples\"),\n",
    "    (\"Sarah wears hats. Sarah wears\", \" hats\"),\n",
    "    (\"The boy found a kite. The boy found a\", \" kite\"),\n",
    "    (\"Alice saw a movie. Alice saw a\", \" movie\"),\n",
    "    (\"He bought milk. He bought\", \" milk\"),\n",
    "    (\"Lisa reads books. Lisa reads\", \" books\"),\n",
    "    (\"A man holds a cane. A man holds a\", \" cane\"),\n",
    "    (\"The child drew a house. The child drew a\", \" house\"),\n",
    "    (\"John met Paul. John met\", \" Paul\"),\n",
    "    (\"Mike drinks coffee. Mike drinks\", \" coffee\"),\n",
    "    (\"Jane picked flowers. Jane picked\", \" flowers\"),\n",
    "    (\"Tom plays chess. Tom plays\", \" chess\"),\n",
    "    (\"Anna wrote a letter. Anna wrote a\", \" letter\"),\n",
    "    (\"She chose a gift. She chose a\", \" gift\"),\n",
    "    (\"David fixed the car. David fixed the\", \" car\"),\n",
    "    (\"Mark visits Paris. Mark visits\", \" Paris\"),\n",
    "    (\"They built a fence. They built a\", \" fence\"),\n",
    "    (\"The cat chased the mouse. The cat chased the\", \" mouse\"),\n",
    "    (\"George paints landscapes. George paints\", \" landscapes\"),\n",
    "    (\"Nancy baked bread. Nancy baked\", \" bread\"),\n",
    "    (\"The cat chased the mouse. The cat chased the\", \" mouse\"),\n",
    "    (\"Alice gave Bob a book. Alice gave Bob a\", \" book\"),\n",
    "    (\"It started to rain in the city. It started to rain in the\", \" city\"),\n",
    "    (\"John picked an apple from the tree. John picked an apple from the\", \" tree\"),\n",
    "    (\"The little boy played with a toy car. The little boy played with a\", \" toy\"),\n",
    "    (\"Sara baked cookies for her friends. Sara baked cookies for her\", \" friends\"),\n",
    "    (\"The dog barked at the stranger. The dog barked at the\", \" stranger\"),\n",
    "    (\"Tom wrote a letter to his mother. Tom wrote a letter to his\", \" mother\"),\n",
    "    (\"The sun set behind the hills. The sun set behind the\", \" hills\"),\n",
    "    (\"Mary wore a red dress to the party. Mary wore a red dress to the\", \" party\"),\n",
    "    (\"They walked along the river bank. They walked along the\", \" river\"),\n",
    "    (\"The chef cooked pasta for dinner. The chef cooked pasta for\", \" dinner\"),\n",
    "    (\"He played guitar on the stage. He played guitar on the\", \" stage\"),\n",
    "    (\"She planted flowers in the garden. She planted flowers in the\", \" garden\"),\n",
    "    (\"The children built a sandcastle on the beach. The children built a sandcastle on the\", \" beach\"),\n",
    "    (\"I bought a ticket to the concert. I bought a ticket to the\", \" concert\"),\n",
    "    (\"They traveled to France last summer. They traveled to\", \" France\"),\n",
    "    (\"The teacher explained the lesson clearly. The teacher explained the\", \" lesson\"),\n",
    "    (\"We watched a movie last night. We watched a\", \" movie\"),\n",
    "    (\"He repaired the broken chair. He repaired the broken\", \" chair\"),\n",
    "    (\"The girl sang beautifully on stage. The girl sang beautifully on\", \" stage\"),\n",
    "    (\"They visited the museum in the afternoon. They visited the\", \" museum\"),\n",
    "    (\"The artist painted a landscape. The artist painted a\", \" landscape\"),\n",
    "    (\"She bought a new phone yesterday. She bought a new\", \" phone\"),\n",
    "    (\"He finished reading the novel. He finished reading the\", \" novel\"),\n",
    "    (\"The bird built a nest in the tree. The bird built a nest in the\", \" tree\"),\n",
    "    (\"They played chess in the park. They played chess in the\", \" park\"),\n",
    "    (\"I made coffee in the morning. I made coffee in the\", \" morning\"),\n",
    "    (\"The team won the championship. The team won the\", \" championship\"),\n",
    "    (\"He adopted a puppy from the shelter. He adopted a puppy from the\", \" shelter\"),\n",
    "    (\"She wrote a poem about love. She wrote a poem about\", \" love\"),\n",
    "    (\"The train arrived at the station. The train arrived at the\", \" station\"),\n",
    "    (\"They danced together all night. They danced together all\", \" night\"),\n",
    "    (\"The baker prepared fresh bread. The baker prepared fresh\", \" bread\"),\n",
    "    (\"He ordered a pizza for lunch. He ordered a pizza for\", \" lunch\"),\n",
    "    (\"The students studied for the exam. The students studied for the\",  \" exam\"),\n",
    "    (\"We listened to music all evening. We listened to\", \" music\"),\n",
    "    (\"She climbed the mountain easily. She climbed the\", \" mountain\"),\n",
    "    (\"He washed the dirty car. He washed the dirty\", \" car\"),\n",
    "    (\"They found shelter from the storm. They found shelter from the\", \" storm\"),\n",
    "]\n",
    "\n",
    "prompts = [p for (p, a) in prompts_data]\n",
    "expected_answers = [a for (p, a) in prompts_data]\n",
    "tokens = model.to_tokens(prompts, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before ablating anything, make sure the full model can actually complete all these prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prompts = []\n",
    "valid_answers = []\n",
    "\n",
    "# Run model on all prompts\n",
    "predicted_answers = run_with_ablation(np.ones(n_heads_total), head_indices, tokens)\n",
    "for prompt, predicted_answer, expected_answer in zip(prompts, predicted_answers, expected_answers):\n",
    "    success = (predicted_answer==expected_answer)\n",
    "    if success: # Keep prompt if success\n",
    "        valid_prompts.append(prompt)\n",
    "        valid_answers.append(expected_answer)\n",
    "    else:\n",
    "        print(f'Model failed with prompt \"{prompt}\"')\n",
    "\n",
    "print(f'{len(prompts)-len(valid_prompts)}/{len(prompts)} prompts rejected because even the full model does not complete them successfully')\n",
    "\n",
    "# Replace prompts by only the good ones\n",
    "prompts = valid_prompts\n",
    "expected_answers = valid_answers\n",
    "\n",
    "# Tokenize prompts\n",
    "tokens = model.to_tokens(prompts, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can run Monte Carlo sampling to identify important heads and exclusion heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 10000 # number of trials to run for each metric\n",
    "keep_full_results = True # turn off if you don't want full data to be stored for later use\n",
    "\n",
    "# Run with many heads ablated to find important heads\n",
    "fraction_of_heads_to_retain = 0.30 # retain 30% of heads on average\n",
    "if DUMP_MONTE_CARLO_RESULTS: results__many_ablated = []\n",
    "importance_scores = np.zeros(n_heads_total)\n",
    "success_count = 0\n",
    "for _ in tqdm(range(n_trials)):\n",
    "    mask = np.random.binomial(1, p=fraction_of_heads_to_retain, size=n_heads_total)\n",
    "    predicted_answers = run_with_ablation(mask, head_indices, tokens)\n",
    "    for prompt_index, (predicted_answer, expected_answer) in enumerate(zip(predicted_answers, expected_answers)):\n",
    "        success = (predicted_answer==expected_answer)\n",
    "        if keep_full_results: results__many_ablated.append((mask, success, prompt_index))\n",
    "        if success:\n",
    "            importance_scores += mask\n",
    "            success_count += 1\n",
    "importance_scores /= importance_scores.sum()\n",
    "print(f'Number of successes (for estimating importance scores): {success_count} out of {n_trials*len(tokens)} samples ({round(100*success_count/(n_trials*len(tokens)))}%)')\n",
    "\n",
    "# Run with few heads ablated to find exclusion heads\n",
    "fraction_of_heads_to_retain = 0.85 # retain 85% of heads on average\n",
    "if DUMP_MONTE_CARLO_RESULTS: results__few_ablated = []\n",
    "exclusion_penalty = np.zeros(n_heads_total)\n",
    "failure_count = 0\n",
    "for _ in tqdm(range(n_trials)):\n",
    "    mask = np.random.binomial(1, p=fraction_of_heads_to_retain, size=n_heads_total)\n",
    "    predicted_answers = run_with_ablation(mask, head_indices, tokens)\n",
    "    for prompt_index, (predicted_answer, expected_answer) in enumerate(zip(predicted_answers, expected_answers)):\n",
    "        success = (predicted_answer==expected_answer)\n",
    "        if keep_full_results: results__few_ablated.append((mask, success, prompt_index))\n",
    "        if not success:\n",
    "            exclusion_penalty += (1-mask)\n",
    "            failure_count += 1\n",
    "exclusion_penalty /= exclusion_penalty.sum()\n",
    "print(f'Number of failures (for estimating exclusion penalty): {failure_count} out of {n_trials*len(tokens)} samples ({round(100*failure_count/(n_trials*len(tokens)))}%)')\n",
    "\n",
    "# Dump data for future use\n",
    "if DUMP_MONTE_CARLO_RESULTS:\n",
    "    data_to_save = {\n",
    "        \"results__many_ablated\": results__many_ablated,\n",
    "        \"results__few_ablated\": results__few_ablated,\n",
    "        \"model_name\": model_name,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_heads\": n_heads,\n",
    "    }\n",
    "    file_path = \"stochastic_ablation__monte_carlo_results.pkl\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(data_to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot importance scores and exclusion penalties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of importance scores and exclusion penalties\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(importance_scores, marker='o', color='green')\n",
    "plt.plot(exclusion_penalty, marker='o', color='red')\n",
    "plt.xlabel(\"Head index\")\n",
    "plt.ylabel(\"Normalized importance\")\n",
    "plt.title(\"Estimated importance (green) and exclusion penalty (red) of attention heads for induction task\")\n",
    "plt.show()\n",
    "\n",
    "# Reshape scores into (n_layers, n_heads) for a heatmap\n",
    "importance_scores_matrix = importance_scores.reshape(n_layers, n_heads)\n",
    "exclusion_penalty_matrix = exclusion_penalty.reshape(n_layers, n_heads)\n",
    "\n",
    "# Create heatmap for importance score\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(importance_scores_matrix, cmap=\"Greens\", annot=True, fmt=\".3f\",\n",
    "            xticklabels=[f\"H{h}\" for h in range(n_heads)],\n",
    "            yticklabels=[f\"L{l}\" for l in range(n_layers)])\n",
    "plt.title(\"Importance Score Heatmap\\n(Probability of success when head is included)\")\n",
    "plt.xlabel(\"Head\")\n",
    "plt.ylabel(\"Layer\")\n",
    "plt.show()\n",
    "\n",
    "# Create heatmap for exclusion penalty\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(exclusion_penalty_matrix, cmap=\"Reds\", annot=True, fmt=\".3f\",\n",
    "            xticklabels=[f\"H{h}\" for h in range(n_heads)],\n",
    "            yticklabels=[f\"L{l}\" for l in range(n_layers)])\n",
    "plt.title(\"Exclusion Penalty Heatmap\\n(Probability of failure when head is ablated)\")\n",
    "plt.xlabel(\"Head\")\n",
    "plt.ylabel(\"Layer\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A scatter plot is helpful to find which heads are outliers -- larger than typical importance scores or exclusion penalties. These are the heads crucial for the model to work successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of importance score vvs exclusion penalty\n",
    "importance_scores_flat = importance_scores_matrix.flatten()\n",
    "exclusion_penalty_flat = exclusion_penalty_matrix.flatten()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(importance_scores_flat, exclusion_penalty_flat, alpha=0.7)\n",
    "\n",
    "# Find outliers\n",
    "percentile_threshold_value = 88\n",
    "mean_importance_scores = np.mean(importance_scores)\n",
    "mean_exclusion_penalty = np.mean(exclusion_penalty)\n",
    "distance_importance_scores = importance_scores - mean_importance_scores\n",
    "distance_exclusion_penalty = exclusion_penalty - mean_exclusion_penalty\n",
    "threshold_importance_score = np.percentile(distance_importance_scores, percentile_threshold_value)\n",
    "threshold_exclusion_penalty = np.percentile(distance_exclusion_penalty, percentile_threshold_value)\n",
    "outlier_indices = np.where((distance_importance_scores > threshold_importance_score) | (distance_exclusion_penalty > threshold_exclusion_penalty))[0]\n",
    "outlier_heads = [head_indices[idx] for idx in outlier_indices]\n",
    "for (l,h) in outlier_heads:\n",
    "    idx = l * n_heads + h\n",
    "    plt.scatter(importance_scores_flat[idx], exclusion_penalty_flat[idx], color='red', s=100)\n",
    "    plt.text(importance_scores_flat[idx], exclusion_penalty_flat[idx]+0.0005, f\"({l},{h})\", fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Exclusion Penalty\")\n",
    "plt.title(\"Importance vs Exclusion for Heads\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f'Number of identified outlier heads that are important for model success: {len(outlier_heads)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see if any of the identified outliers match heads known from prior works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some known heads (e.g., LLM Circuit Analyses Are Consistent Across Training and Scale, Tigges et al. (2024) arXiv:2407.10827)\n",
    "known_induction_heads = [(4,11), (4,6), (5,0)]\n",
    "\n",
    "print(\"Some known induction head(s):\", known_induction_heads)\n",
    "print(\"Found outlier head(s):\", outlier_heads)\n",
    "\n",
    "overlap = set(known_induction_heads).intersection(outlier_heads)\n",
    "print(f\"Overlap: {len(overlap)} outlier head(s) matched out of {len(known_induction_heads)} known: {overlap}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can run models that only retain, or only ablate, the identified outlier heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_retained = np.isin(np.arange(n_heads_total), outlier_indices).astype(int)\n",
    "mask_ablated = 1-np.isin(np.arange(n_heads_total), outlier_indices).astype(int)\n",
    "mask_full = np.ones(len(importance_scores)).astype(int)\n",
    "success_retained = 0\n",
    "success_ablated = 0\n",
    "success_full = 0\n",
    "predicted_answers_retained = run_with_ablation(mask_retained, head_indices, tokens)\n",
    "predicted_answers_ablated = run_with_ablation(mask_ablated, head_indices, tokens)\n",
    "predicted_answers_full = run_with_ablation(mask_full, head_indices, tokens)\n",
    "for prompt, expected_answer, predicted_answer_retained, predicted_answer_ablated, predicted_answer_full in zip(prompts, expected_answers, predicted_answers_retained, predicted_answers_ablated, predicted_answers_full):\n",
    "    success_retained += (predicted_answer_retained==expected_answer)\n",
    "    success_ablated += (predicted_answer_ablated==expected_answer)\n",
    "    success_full += (predicted_answer_full==expected_answer)\n",
    "\n",
    "\n",
    "# Plot result for the 3 models\n",
    "results = {\n",
    "    f'Outlier heads kept only\\n({len(outlier_indices)} of {n_heads_total} heads used)': (success_retained, len(tokens)-success_retained),   # (correct, fail)\n",
    "    f'Outlier heads ablated\\n({n_heads_total-len(outlier_indices)} of {n_heads_total} heads used)': (success_ablated, len(tokens)-success_ablated),\n",
    "    f'Full model\\n({n_heads_total} of {n_heads_total} heads used)': (success_full, len(tokens)-success_full)\n",
    "}\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for ax, (title, (correct, fail)) in zip(axes, results.items()):\n",
    "    # Prepare data, labels and colors\n",
    "    sizes = []\n",
    "    labels = []\n",
    "    colors = []\n",
    "    if correct > 0:\n",
    "        sizes.append(correct)\n",
    "        labels.append(f\"Correct ({correct})\")\n",
    "        colors.append(\"#4CAF50\")\n",
    "    if fail > 0:\n",
    "        sizes.append(fail)\n",
    "        labels.append(f\"Fail ({fail})\")\n",
    "        colors.append(\"#F44336\")\n",
    "\n",
    "    # Plot pie\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        sizes,\n",
    "        labels=labels,\n",
    "        autopct='%1.1f%%',\n",
    "        colors=colors,\n",
    "        startangle=90,\n",
    "        wedgeprops=dict(edgecolor='w')\n",
    "    )\n",
    "    for text in texts + autotexts:\n",
    "        text.set_fontsize(12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "plt.suptitle(\"Fraction of Correctly Completed Tasks Under Different Ablation Strategies\", fontsize=16, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Correctly completed tasks:')\n",
    "print(f'    {success_retained}/{len(prompts)} = {int(100*success_retained/len(prompts))}% (model with only outlier heads retained)')\n",
    "print(f'    {success_ablated}/{len(prompts)} = {int(100*success_ablated/len(prompts))}% (model with outlier heads ablated)')\n",
    "print(f'    {success_full}/{len(prompts)} = {int(100*success_full/len(prompts))}% (full model)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "> In this project, we explored how to identify a minimal set of attention heads required for a small transformer model (pythia-160m) to reproduce a known induction pattern (IOI).\n",
    ">Using extensive Monte Carlo sampling â€” totaling over 1.1 million ablation trials â€” we assessed both the importance of heads (via high-ablation runs) and their criticality (via low-ablation exclusion tests). This analysis uncovered just 27 attention heads (~19% of the total) that consistently exhibited outsized importance scores or exclusion penalties.\n",
    ">Notably, we recovered the well-known induction mover head (5,0), validating our approach, and identified head (3,2) as uniquely critical, topping both importance and exclusion metrics.\n",
    ">Retaining only these 27 heads preserved task success at roughly 90%, nearly matching the full modelâ€™s performance. In contrast, ablating only these heads caused task success to plummet to less than 2%, revealing how transformer circuits can be simultaneously robust and sharply dependent on small functional pathways.\n",
    "\n",
    "### Key Figures\n",
    "\n",
    "- **Scatter plot:** shows importance vs exclusion penalty for all heads, highlighting identified outliers.\n",
    "\n",
    "![Scatter Plot](https://github.com/thartlep/stochastic-ablation/raw/main/figures/scatter_plot.png)\n",
    "\n",
    "- **Pie charts:** compare success rates across three scenarios â€” \n",
    "  retaining only the 27 critical heads, ablating only these heads, and the full model.\n",
    "\n",
    "![Pie Charts](https://github.com/thartlep/stochastic-ablation/raw/main/figures/pie_charts.png)\n",
    "\n",
    "### Key Takeaways\n",
    "- Only **27 heads (~19%)** are needed to preserve ~90% task performance.\n",
    "- Known induction mover **(5,0)** is recovered; **(3,2)** is uniquely critical.\n",
    "- Ablating these heads collapses success to **~1.8%**, exposing sharp vulnerabilities.\n",
    "- Demonstrates how transformer circuits are both redundant and sharply localized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Directions: Beyond Single-Head Importance\n",
    "\n",
    "> **Where next?**\n",
    ">\n",
    "> This study identified key attention heads critical for reproducing a known induction pattern (IOI) using stochastic ablation.  \n",
    "> A natural extension is to explore **conditional probabilities**:\n",
    ">\n",
    "> - *Given that head A is present, how essential is head B?*  \n",
    "> - *Which heads fail together, or succeed only jointly?*\n",
    ">\n",
    "> By building a graph of these dependencies, we could map out **circuits as branching substructures**, tracing how the transformer coordinates multiple heads across layers to achieve complex tasks.\n",
    ">\n",
    "> For single-token predictions, heads within a layer operate independently.  \n",
    "> But in **multi-token generation**, outputs feed back as inputs â€” letting heads in the same layer influence each other across time steps, weaving even richer recurrent circuit trees.\n",
    ">\n",
    "> ðŸ“ˆ *Stay tuned for a follow-up notebook diving into these conditional, multi-head relationships!*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
