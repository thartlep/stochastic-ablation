{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Ablation to Investigate Minimal Causal Sets in Transformers\n",
    "This notebook explores how to identify a minimal set of attention heads needed\n",
    "for a small transformer model to reproduce a known induction pattern (IOI). We do this via\n",
    "Monte Carlo sampling by randomly ablating heads and tracking which heads are consistently\n",
    "required for success (important heads), and which heads consistently lead to failure when ablated (exclusion heads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, import needed Python modules and load small transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore mostly harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Import needed modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Environment safety\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Load small model\n",
    "model = HookedTransformer.from_pretrained(\"pythia-160m\", device=\"cpu\")\n",
    "head_indices = [(l,h) for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)]\n",
    "n_heads_total = len(head_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And define a helper function to run the model with an ablation mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_ablation(sample_mask, head_indices, tokens):\n",
    "    \"\"\"\n",
    "    Given a binary mask and list of head indices, run the model with\n",
    "    the masked heads zeroed out.\n",
    "    \"\"\"\n",
    "    heads_to_ablate = [head_indices[i] for i, keep in enumerate(sample_mask) if not keep]\n",
    "\n",
    "    def hook_ablate(value, hook):\n",
    "        # value shape: (zbatch, seq, n_heads, d_head)\n",
    "        for i, (layer_idx, head_idx) in enumerate(head_indices):\n",
    "            if (layer_idx, head_idx) in heads_to_ablate and f\"blocks.{layer_idx}.attn.hook_z\" == hook.name:\n",
    "                value[:,:,head_idx,:] = 0.0\n",
    "        return value\n",
    "\n",
    "    hooks = [(f\"blocks.{layer}.attn.hook_z\", hook_ablate) for layer, head in heads_to_ablate]\n",
    "    logits = model.run_with_hooks(tokens, return_type=\"logits\", fwd_hooks=hooks)\n",
    "\n",
    "    seq_lens = (tokens != model.tokenizer.pad_token_id).sum(dim=1)\n",
    "\n",
    "    next_logits = logits[torch.arange(len(prompts)), seq_lens, :]\n",
    "    pred_ids = next_logits.argmax(dim=-1)\n",
    "    predicted_answers = [model.to_string(pid.unsqueeze(0)) for pid in pred_ids]\n",
    "\n",
    "    return predicted_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now setup some test prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_data = [\n",
    "    (\"Tom eats apples. Tom eats\", \" apples\"),\n",
    "    (\"Sarah wears hats. Sarah wears\", \" hats\"),\n",
    "    (\"The boy found a kite. The boy found a\", \" kite\"),\n",
    "    (\"Alice saw a movie. Alice saw a\", \" movie\"),\n",
    "    (\"He bought milk. He bought\", \" milk\"),\n",
    "    (\"Lisa reads books. Lisa reads\", \" books\"),\n",
    "    (\"A man holds a cane. A man holds a\", \" cane\"),\n",
    "    (\"The child drew a house. The child drew a\", \" house\"),\n",
    "    (\"John met Paul. John met\", \" Paul\"),\n",
    "    (\"Mike drinks coffee. Mike drinks\", \" coffee\"),\n",
    "    (\"Jane picked flowers. Jane picked\", \" flowers\"),\n",
    "    (\"Tom plays chess. Tom plays\", \" chess\"),\n",
    "    (\"Anna wrote a letter. Anna wrote a\", \" letter\"),\n",
    "    (\"She chose a gift. She chose a\", \" gift\"),\n",
    "    (\"David fixed the car. David fixed the\", \" car\"),\n",
    "    (\"Mark visits Paris. Mark visits\", \" Paris\"),\n",
    "    (\"They built a fence. They built a\", \" fence\"),\n",
    "    (\"The cat chased the mouse. The cat chased the\", \" mouse\"),\n",
    "    (\"George paints landscapes. George paints\", \" landscapes\"),\n",
    "    (\"Nancy baked bread. Nancy baked\", \" bread\"),\n",
    "    (\"The cat chased the mouse. The cat chased the\", \" mouse\"),\n",
    "    (\"Alice gave Bob a book. Alice gave Bob a\", \" book\"),\n",
    "    (\"It started to rain in the city. It started to rain in the\", \" city\"),\n",
    "    (\"John picked an apple from the tree. John picked an apple from the\", \" tree\"),\n",
    "    (\"The little boy played with a toy car. The little boy played with a\", \" toy\"),\n",
    "    (\"Sara baked cookies for her friends. Sara baked cookies for her\", \" friends\"),\n",
    "    (\"The dog barked at the stranger. The dog barked at the\", \" stranger\"),\n",
    "    (\"Tom wrote a letter to his mother. Tom wrote a letter to his\", \" mother\"),\n",
    "    (\"The sun set behind the hills. The sun set behind the\", \" hills\"),\n",
    "    (\"Mary wore a red dress to the party. Mary wore a red dress to the\", \" party\"),\n",
    "    (\"They walked along the river bank. They walked along the\", \" river\"),\n",
    "    (\"The chef cooked pasta for dinner. The chef cooked pasta for\", \" dinner\"),\n",
    "    (\"He played guitar on the stage. He played guitar on the\", \" stage\"),\n",
    "    (\"She planted flowers in the garden. She planted flowers in the\", \" garden\"),\n",
    "    (\"The children built a sandcastle on the beach. The children built a sandcastle on the\", \" beach\"),\n",
    "    (\"I bought a ticket to the concert. I bought a ticket to the\", \" concert\"),\n",
    "    (\"They traveled to France last summer. They traveled to\", \" France\"),\n",
    "    (\"The teacher explained the lesson clearly. The teacher explained the\", \" lesson\"),\n",
    "    (\"We watched a movie last night. We watched a\", \" movie\"),\n",
    "    (\"He repaired the broken chair. He repaired the broken\", \" chair\"),\n",
    "    (\"The girl sang beautifully on stage. The girl sang beautifully on\", \" stage\"),\n",
    "    (\"They visited the museum in the afternoon. They visited the\", \" museum\"),\n",
    "    (\"The artist painted a landscape. The artist painted a\", \" landscape\"),\n",
    "    (\"She bought a new phone yesterday. She bought a new\", \" phone\"),\n",
    "    (\"He finished reading the novel. He finished reading the\", \" novel\"),\n",
    "    (\"The bird built a nest in the tree. The bird built a nest in the\", \" tree\"),\n",
    "    (\"They played chess in the park. They played chess in the\", \" park\"),\n",
    "    (\"I made coffee in the morning. I made coffee in the\", \" morning\"),\n",
    "    (\"The team won the championship. The team won the\", \" championship\"),\n",
    "    (\"He adopted a puppy from the shelter. He adopted a puppy from the\", \" shelter\"),\n",
    "    (\"She wrote a poem about love. She wrote a poem about\", \" love\"),\n",
    "    (\"The train arrived at the station. The train arrived at the\", \" station\"),\n",
    "    (\"They danced together all night. They danced together all\", \" night\"),\n",
    "    (\"The baker prepared fresh bread. The baker prepared fresh\", \" bread\"),\n",
    "    (\"He ordered a pizza for lunch. He ordered a pizza for\", \" lunch\"),\n",
    "    (\"The students studied for the exam. The students studied for the\",  \" exam\"),\n",
    "    (\"We listened to music all evening. We listened to\", \" music\"),\n",
    "    (\"She climbed the mountain easily. She climbed the\", \" mountain\"),\n",
    "    (\"He washed the dirty car. He washed the dirty\", \" car\"),\n",
    "    (\"They found shelter from the storm. They found shelter from the\", \" storm\"),\n",
    "]\n",
    "\n",
    "prompts = [p for (p, a) in prompts_data]\n",
    "expected_answers = [a for (p, a) in prompts_data]\n",
    "tokens = model.to_tokens(prompts, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before ablating anything, make sure the full model can actually complete all these prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prompts = []\n",
    "valid_answers = []\n",
    "\n",
    "# Run model on all prompts\n",
    "predicted_answers = run_with_ablation(np.ones(n_heads_total), head_indices, tokens)\n",
    "for prompt, predicted_answer, expected_answer in zip(prompts, predicted_answers, expected_answers):\n",
    "    success = (predicted_answer==expected_answer)\n",
    "    if success: # Keep prompt if success\n",
    "        valid_prompts.append(prompt)\n",
    "        valid_answers.append(expected_answer)\n",
    "    else:\n",
    "        print(f'Model failed with prompt \"{prompt}\"')\n",
    "\n",
    "print(f'{len(prompts)-len(valid_prompts)}/{len(prompts)} prompts rejected because even the full model does not complete them successfully')\n",
    "\n",
    "# Replace prompts by only the good ones\n",
    "prompts = valid_prompts\n",
    "expected_answers = valid_answers\n",
    "\n",
    "# Tokenize prompts\n",
    "tokens = model.to_tokens(prompts, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can run Monte Carlo sampling to identify important heads and exclusion heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "\n",
    "# Run with many heads ablated to find important heads\n",
    "fraction_of_heads_to_retain = 0.30 # retain 30% of heads on average\n",
    "results__many_ablated = []\n",
    "importance_scores = np.zeros(n_heads_total)\n",
    "success_count = 0\n",
    "for _ in tqdm(range(n_trials)):\n",
    "    mask = np.random.binomial(1, p=fraction_of_heads_to_retain, size=n_heads_total)\n",
    "    predicted_answers = run_with_ablation(mask, head_indices, tokens)\n",
    "    for prompt_index, (predicted_answer, expected_answer) in enumerate(zip(predicted_answers, expected_answers)):\n",
    "        success = (predicted_answer==expected_answer)\n",
    "        results__many_ablated.append((mask, success, prompt_index))\n",
    "        if success:\n",
    "            importance_scores += mask\n",
    "            success_count += 1\n",
    "importance_scores /= importance_scores.sum()\n",
    "print(f'Number of successes (for estimating importance scores): {success_count} out of {len(results__many_ablated)} samples ({round(success_count/len(results__many_ablated)*100)}%)')\n",
    "\n",
    "# Run with few heads ablated to find exclusion heads\n",
    "fraction_of_heads_to_retain = 0.85 # retain 85% of heads on average\n",
    "results__few_ablated = []\n",
    "exclusion_penalty = np.zeros(n_heads_total)\n",
    "failure_count = 0\n",
    "for _ in tqdm(range(n_trials)):\n",
    "    mask = np.random.binomial(1, p=fraction_of_heads_to_retain, size=n_heads_total)\n",
    "    predicted_answers = run_with_ablation(mask, head_indices, tokens)\n",
    "    for prompt_index, (predicted_answer, expected_answer) in enumerate(zip(predicted_answers, expected_answers)):\n",
    "        success = (predicted_answer==expected_answer)\n",
    "        results__few_ablated.append((mask, success, prompt_index))\n",
    "        if not success:\n",
    "            exclusion_penalty += (1-mask)\n",
    "            failure_count += 1\n",
    "exclusion_penalty /= exclusion_penalty.sum()\n",
    "print(f'Number of failures (for estimating exclusion penalty): {failure_count} out of {len(results__few_ablated)} samples ({round(failure_count/len(results__many_ablated)*100)}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot importance scores and exclusion penalties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of importance scores and exclusion penalties\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(importance_scores, marker='o', color='green')\n",
    "plt.plot(exclusion_penalty, marker='o', color='red')\n",
    "plt.xlabel(\"Head index\")\n",
    "plt.ylabel(\"Normalized importance\")\n",
    "plt.title(\"Estimated importance (green) and exclusion penalty (red) of attention heads for induction task\")\n",
    "plt.show()\n",
    "\n",
    "# Reshape scores into (n_layers, n_heads) for a heatmap\n",
    "n_layers = model.cfg.n_layers\n",
    "n_heads = model.cfg.n_heads\n",
    "importance_scores_matrix = importance_scores.reshape(n_layers, n_heads)\n",
    "exclusion_penalty_matrix = exclusion_penalty.reshape(n_layers, n_heads)\n",
    "\n",
    "# Create heatmap for importance score\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(importance_scores_matrix, cmap=\"Greens\", annot=True, fmt=\".3f\",\n",
    "            xticklabels=[f\"H{h}\" for h in range(n_heads)],\n",
    "            yticklabels=[f\"L{l}\" for l in range(n_layers)])\n",
    "plt.title(\"Importance Score Heatmap\\n(Probability of success when head is included)\")\n",
    "plt.xlabel(\"Head\")\n",
    "plt.ylabel(\"Layer\")\n",
    "plt.show()\n",
    "\n",
    "# Create heatmap for exclusion penalty\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(exclusion_penalty_matrix, cmap=\"Reds\", annot=True, fmt=\".3f\",\n",
    "            xticklabels=[f\"H{h}\" for h in range(n_heads)],\n",
    "            yticklabels=[f\"L{l}\" for l in range(n_layers)])\n",
    "plt.title(\"Exclusion Penalty Heatmap\\n(Probability of failure when head is ablated)\")\n",
    "plt.xlabel(\"Head\")\n",
    "plt.ylabel(\"Layer\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A scatter plot is helpful to find which heads are outliers -- larger than typical importance scores or exclusion penalties. These are the heads crucial for the model to work successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot of importance score vvs exclusion penalty\n",
    "importance_scores_flat = importance_scores_matrix.flatten()\n",
    "exclusion_penalty_flat = exclusion_penalty_matrix.flatten()\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(importance_scores_flat, exclusion_penalty_flat, alpha=0.7)\n",
    "\n",
    "# Find outliers\n",
    "percentile_threshold_value = 90\n",
    "mean_importance_scores = np.mean(importance_scores)\n",
    "mean_exclusion_penalty = np.mean(exclusion_penalty)\n",
    "distance_importance_scores = importance_scores - mean_importance_scores\n",
    "distance_exclusion_penalty = exclusion_penalty - mean_exclusion_penalty\n",
    "threshold_importance_score = np.percentile(distance_importance_scores, percentile_threshold_value)\n",
    "threshold_exclusion_penalty = np.percentile(distance_exclusion_penalty, percentile_threshold_value)\n",
    "outlier_indices = np.where((distance_importance_scores > threshold_importance_score) | (distance_exclusion_penalty > threshold_exclusion_penalty))[0]\n",
    "outlier_heads = [head_indices[idx] for idx in outlier_indices]\n",
    "for (l,h) in outlier_heads:\n",
    "    idx = l * n_heads + h\n",
    "    plt.scatter(importance_scores_flat[idx], exclusion_penalty_flat[idx], color='red', s=100)\n",
    "    plt.text(importance_scores_flat[idx], exclusion_penalty_flat[idx]+0.0005, f\"({l},{h})\", fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Exclusion Penalty\")\n",
    "plt.title(\"Importance vs Exclusion for Heads\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f'Number of identified outlier heads that are important for model success: {len(outlier_heads)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see if any of the identified outliers match heads known from prior works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some known heads (e.g., LLM Circuit Analyses Are Consistent Across Training and Scale, Tigges et al. (2024) arXiv:2407.10827)\n",
    "known_induction_heads = [(4,11), (4,6), (5,0)]\n",
    "\n",
    "print(\"Some known induction head(s):\", known_induction_heads)\n",
    "print(\"Found outlier head(s):\", outlier_heads)\n",
    "\n",
    "overlap = set(known_induction_heads).intersection(outlier_heads)\n",
    "print(f\"Overlap: {len(overlap)} outlier head(s) matched out of {len(known_induction_heads)} known: {overlap}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can run models that only retain, or only ablate, the identified outlier heads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_retained = np.isin(np.arange(n_heads_total), outlier_indices).astype(int)\n",
    "mask_ablated = 1-np.isin(np.arange(n_heads_total), outlier_indices).astype(int)\n",
    "mask_full = np.ones(len(importance_scores)).astype(int)\n",
    "success_retained = 0\n",
    "success_ablated = 0\n",
    "success_full = 0\n",
    "predicted_answers_retained = run_with_ablation(mask_retained, head_indices, tokens)\n",
    "predicted_answers_ablated = run_with_ablation(mask_ablated, head_indices, tokens)\n",
    "predicted_answers_full = run_with_ablation(mask_full, head_indices, tokens)\n",
    "for prompt, expected_answer, predicted_answer_retained, predicted_answer_ablated, predicted_answer_full in zip(prompts, expected_answers, predicted_answers_retained, predicted_answers_ablated, predicted_answers_full):\n",
    "    success_retained += (predicted_answer_retained==expected_answer)\n",
    "    success_ablated += (predicted_answer_ablated==expected_answer)\n",
    "    success_full += (predicted_answer_full==expected_answer)\n",
    " \n",
    "print(f'Correctly completed tasks:')\n",
    "print(f'    {success_retained}/{len(prompts)} (model with only outlier heads retained)')\n",
    "print(f'    {success_ablated}/{len(prompts)} (model with outlier heads ablated)')\n",
    "print(f'    {success_full}/{len(prompts)} (full model)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
